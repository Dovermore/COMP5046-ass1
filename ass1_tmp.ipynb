{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/Dovermore/COMP5046-ass1/blob/master/zhua9812_COMP5046_Ass1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# COMP5046 Assignment 1\n",
    "*Make sure you change the file name with your unikey.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Readme\n",
    "*If there is something to be noted for the user, please mention here.*\n",
    "\n",
    "*If you are planning to implement a program with Object Oriented Programming style*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Visualising the comparison of different results is a good way to justify your decision.***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 - Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1. Download Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Code to download file into Colaboratory:\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "id = '1vF3FqgBC1Y-RPefeVmY8zetdZG1jmHzT'\n",
    "downloaded = drive.CreateFile({'id':id})\n",
    "downloaded.GetContentFile('imdb_train.csv')\n",
    "\n",
    "id = '1XhaV8YMuQeSwozQww8PeyiWMJfia13G6'\n",
    "downloaded = drive.CreateFile({'id':id})\n",
    "downloaded.GetContentFile('imdb_test.csv')\n",
    "\n",
    "import pandas as pd\n",
    "df_train = pd.read_csv(\"imdb_train.csv\")\n",
    "df_test = pd.read_csv(\"imdb_test.csv\")\n",
    "\n",
    "reviews_train = df_train['review'].tolist()\n",
    "sentiments_train = df_train['sentiment'].tolist()\n",
    "reviews_test = df_test['review'].tolist()\n",
    "sentiments_test = df_test['sentiment'].tolist()\n",
    "\n",
    "print(\"Training set number:\",len(reviews_train))\n",
    "print(\"Testing set number:\",len(reviews_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2. Preprocess data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*You are required to describe which data preprocessing techniques were conducted with justification of your decision. *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install contractions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Please comment your code\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "import copy\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords as sw\n",
    "\n",
    "# eng_stopwords = sw.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def remove_punctuation(x):\n",
    "    \"\"\"\n",
    "    Remove all non white space or word character in function x\n",
    "    :param x: The sentence to process\n",
    "    :return: str with all non white space or word character removed\n",
    "    \"\"\"\n",
    "    x = re.sub(r'[^\\w\\s]','',x)\n",
    "    return x\n",
    "\n",
    "def preprocess_texts(X, rm_htmltag=True, expand_contraction=True, to_lower=True, rm_punctuation=True,\n",
    "                     lemmatize=True):\n",
    "    \"\"\"\n",
    "    Preprocess texts with the specified preprocessing procedures\n",
    "    :param X: A list of texts to be processed\n",
    "    :param rm_htmltag: If html tags should be removed\n",
    "    :param expand_contraction: If contraction should be expanded\n",
    "    :param to_lower: If cases should be converted to lower case\n",
    "    :param rm_punctuation: If punctuation should be removed\n",
    "    :param lemmatize: If tokens should be lemmatized\n",
    "    :return: list[list[processed token]]\n",
    "    \"\"\"\n",
    "    if rm_htmltag:\n",
    "        # Use beautiful soup to remove html tags if any\n",
    "        X = [BeautifulSoup(s).get_text() for s in X]\n",
    "\n",
    "    if expand_contraction:\n",
    "        # expand contactions (english only) to normalise text (this before lower case because this will give uppercase)\n",
    "        X = [contractions.fix(s) for s in X]\n",
    "\n",
    "    if to_lower:\n",
    "        # Case folding is necessary to reduce the unique words and removing some irregular case formulation for words.\n",
    "        # Though this may cause the loss of some information (for instance, all CAPPED words have strong emotion),\n",
    "        # it is generally beneficial to smooth the occurances of words\n",
    "        X = [s.lower() for s in X]\n",
    "\n",
    "    if rm_punctuation:\n",
    "        # Remove punctuations is necessary for almost the same reason as the case folding. Here because each tweet is self\n",
    "        # contained, no need to add end of sentence token.\n",
    "        X = [remove_punctuation(s) for s in X]\n",
    "\n",
    "    # Tokenization is necessary to extract each individual words instead of feeding in raw sentences.\n",
    "    X = [word_tokenize(sent) for sent in X]\n",
    "\n",
    "    # Stop words are NOT removed (yet) for they sometimes affect the sentiment by a lot (like word not, wouldn't)\n",
    "    # If I can get better list and spend more time understanding the data then I will remove them\n",
    "\n",
    "    if lemmatize:\n",
    "        # Lemmatise tokens to reduce the number of unique words, and make the training process easier by reducing the labels\n",
    "        X = [[lemmatizer.lemmatize(w) for w in tokens] for tokens in X]\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "class TextPreprocessTransformer(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Simple transformer class to wrap the previous transformation\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return preprocess_texts(X, **self.kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tpt = TextPreprocessTransformer()\n",
    "texts_train = tpt.fit_transform(reviews_train)\n",
    "texts_test = tpt.fit_transform(reviews_test)\n",
    "print(texts_train[:2])\n",
    "print(texts_test[:2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_train = label_encoder.fit_transform(sentiments_train)\n",
    "label_test = label_encoder.transform(sentiments_test)\n",
    "print(label_train[:50])\n",
    "print(label_test[:50])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 - Model Implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1. Word Embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*You are required to describe which model was implemented (i.e. Word2Vec with CBOW, FastText with SkipGram, etc.) with justification of your decision *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.1. Data Preprocessing for Word Embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*You are required to describe which preprocessing techniques were used with justification of your decision.*\n",
    "\n",
    "**Important**: If you are going to use the code from lab3 word2vec preprocessing. Please note that `word_list = list(set(word_list)) ` has randomness. So to make sure the word_list is the same every time you run it, you can put `word_list.sort()` after that line of code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# The following code implements Word2Vec with Skip-gram. For it is simple and straightforward to implement and there\n",
    "# is previous code base in previous labs\n",
    "\n",
    "class VocabTransformer(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Base transformer handling vocabulary related tasks by fitting a vocabulary\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.token_list = []\n",
    "        self.token_dict = {}\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        \"\"\"\n",
    "        Fit this transformer with training data to obtain vocabulary\n",
    "        :param X: Training data to be fitted\n",
    "        :param y: Ignored\n",
    "        :param refit: Specifies if this fit should be a refit (reinitialise vocab list) or build upon previous vocab list\n",
    "        :return: itself for chaining\n",
    "        \"\"\"\n",
    "        refit = fit_params.get(\"refit\", False)\n",
    "        token_set = set()\n",
    "        for tokens in X:\n",
    "            token_set |= set(tokens)\n",
    "        if refit:\n",
    "            self.token_list = list(token_set)\n",
    "        else:\n",
    "            token_set -= set(self.token_list)\n",
    "            self.token_list += list(token_set)\n",
    "        self.token_dict = {w: i for i, w in enumerate(self.token_list)}\n",
    "        return self\n",
    "\n",
    "# First define a dataset generator\n",
    "class SkipGramTransformer(VocabTransformer):\n",
    "    \"\"\"\n",
    "    Transformer class to convert raw list of list of tokens to data to be trained for skipgram model\n",
    "    \"\"\"\n",
    "    def __init__(self, window=10):\n",
    "        \"\"\"\n",
    "        Init this transformer with a given window size for sampling skip grams\n",
    "        :param window: The window size for sampling. Note this is the size of one side,\n",
    "            the total number of context sampled is 2 * window\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.window = window\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Transforms a given dataset (list of list of tokens) to negative gram data (target, context) pair\n",
    "        :param X: The input data\n",
    "        :param y: Ignored\n",
    "        :return: One array of targets and one array of contexts\n",
    "        \"\"\"\n",
    "        skip_grams = []\n",
    "        for tokens in X:\n",
    "            for i in range(len(tokens)):\n",
    "                target = self.token_dict.get(tokens[i], None)\n",
    "                if target is None: continue\n",
    "                for k in range(max(i - self.window, 0), min(i + self.window, len(tokens))):\n",
    "                    if k == i:\n",
    "                        continue\n",
    "                    context = self.token_dict.get(tokens[k], None)\n",
    "                    if context is None: continue\n",
    "                    skip_grams.append([target, context])\n",
    "        X, y = list(zip(*skip_grams))\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def generator(self, X, batch_size=1024, pool_size=5120):\n",
    "        \"\"\"\n",
    "        Generates a generator for transforming a given dataset (list of list of tokens)\n",
    "        to negative gram data (target, context) pair. The generated generator will generate\n",
    "        an array of target words and array of context words both of shape (batch_size, ).\n",
    "        This is needed because the previous way of generating will exceed the memory capacity\n",
    "        :param X: The input data\n",
    "        :param y: Ignored\n",
    "        :return: A generator whose __next__ output the following data\n",
    "                :Array of targets, array of contexts, flag specifying if this is the end of a epoch\n",
    "        \"\"\"\n",
    "        skip_gram_pool = np.zeros((0, 2), dtype=int)\n",
    "        idx = 0\n",
    "        while True:\n",
    "            end_epoch = False\n",
    "            while skip_gram_pool.shape[0] < pool_size:\n",
    "                tokens = X[idx]\n",
    "                skip_grams = []\n",
    "                for i in range(len(tokens)):\n",
    "                    target = self.token_dict.get(tokens[i], None)\n",
    "                    if target is None: continue\n",
    "                    for k in range(max(i - self.window, 0), min(i + self.window, len(tokens))):\n",
    "                        if k == i:\n",
    "                            continue\n",
    "                        context = self.token_dict.get(tokens[k], None)\n",
    "                        if context is None: continue\n",
    "                        skip_grams.append([target, context])\n",
    "                skip_gram_pool = np.concatenate([skip_gram_pool, skip_grams], axis=0)\n",
    "                idx += 1\n",
    "                if idx >= len(X):\n",
    "                    end_epoch = True\n",
    "                idx %= len(X)\n",
    "            batch_idx = np.random.choice(skip_gram_pool.shape[0], size=batch_size, replace=False)\n",
    "            yield skip_gram_pool[batch_idx, 0], skip_gram_pool[batch_idx, 1].reshape(-1), end_epoch\n",
    "            skip_gram_pool = np.delete(skip_gram_pool, batch_idx, axis=0)\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "skip_gram_pipeline = make_pipeline(TextPreprocessTransformer(), SkipGramTransformer())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test\n",
    "sgt = SkipGramTransformer()\n",
    "test_data = np.array(range(10)).reshape(5, 2).tolist()\n",
    "sgt.fit(test_data)\n",
    "i = 0\n",
    "for i, (a, b, c) in enumerate(sgt.generator(test_data, 2, 10)):\n",
    "    print(i, a, b, c)\n",
    "    if i > 20:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Profile time\n",
    "sgt = SkipGramTransformer()\n",
    "sgt.fit(texts_train)\n",
    "datagen = sgt.generator(texts_train)\n",
    "%timeit next(datagen)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.2. Build Word Embeddings Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*You are required to describe how hyperparameters were decided with justification of your decision.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.optim import Adam\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # store optimizer\n",
    "        self.optimizer = None\n",
    "\n",
    "    def train_step(self, X_batch, y_batch):\n",
    "        # zero the parameter gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = self.forward(X_batch)\n",
    "        loss = self.loss_fn(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return outputs, loss\n",
    "\n",
    "    def train(self, data, epochs, batch_size=1024, batch_display_interval=10000, epoch_display_interval=100, data_gen_dict={}):\n",
    "        if batch_display_interval <= 0:\n",
    "            batch_display_interval = 1000000000\n",
    "        if epoch_display_interval <= 0:\n",
    "            epoch_display_interval = 1000000000\n",
    "        data_gen = self.data_generator(data, batch_size=batch_size, **data_gen_dict)\n",
    "        batch = 0\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            epoch_size = 0\n",
    "            for X_batch, y_batch, end_epoch in data_gen:\n",
    "                X_batch = torch.from_numpy(X_batch).to(device)\n",
    "                y_batch = torch.from_numpy(y_batch).to(device)\n",
    "                # Train\n",
    "                outputs, loss = self.train_step(X_batch, y_batch)\n",
    "                epoch_loss += loss * batch_size\n",
    "                epoch_size += batch_size\n",
    "                batch += 1\n",
    "                if batch % batch_display_interval == batch_display_interval:\n",
    "                    print('    Batch: %d, loss: %.4f' %(batch, loss))\n",
    "                if end_epoch:\n",
    "                    break\n",
    "            epoch_loss /= epoch_size\n",
    "            if epoch % epoch_display_interval == epoch_display_interval - 1:\n",
    "                print('Epoch: %d, loss: %.4f' %(epoch + 1, epoch_loss))\n",
    "\n",
    "    def data_generator(self, X, batch_size, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "\n",
    "# Define model\n",
    "class W2VSkipGram(BaseModel):\n",
    "    def __init__(self, num_embeddings, embedding_dim, window=10, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # linear embedding\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        # linear mapping\n",
    "        self.forward_layer = nn.Linear(embedding_dim, num_embeddings, bias=False)\n",
    "        # text transformer\n",
    "        self.skip_gram_transformer = SkipGramTransformer(window)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # forward pass\n",
    "        return self.forward_layer(self.embedding_layer(X))\n",
    "\n",
    "    def data_generator(self, X, batch_size, **kwargs):\n",
    "        # Fit vocab only if training\n",
    "        if self.training:\n",
    "            self.skip_gram_transformer.fit(X)\n",
    "        return self.skip_gram_transformer.generator(X, batch_size=batch_size, **kwargs)\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return output.argmax(dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data = np.array(range(4)).reshape(2, 2).tolist()\n",
    "test_model = W2VSkipGram(4, 4).to(device)\n",
    "optimizer = Adam(test_model.parameters())\n",
    "test_model.set_optimizer(optimizer)\n",
    "datagen = test_model.data_generator(test_data, batch_size=2, pool_size=4)\n",
    "next(datagen)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_X = torch.from_numpy(np.array([0, 1, 2, 3])).to(device)\n",
    "_y = torch.from_numpy(np.array([1, 0, 3, 2]))\n",
    "# Before\n",
    "print(test_model.forward(_X), test_model.predict(_X))\n",
    "test_model.train(test_data, 1000, 2, batch_display_interval=0, epoch_display_interval=0, data_gen_dict={\"pool_size\":2})\n",
    "# After\n",
    "# Overfit the small dataset\n",
    "print(test_model.forward(_X), test_model.predict(_X))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# profile step time\n",
    "sgt = SkipGramTransformer().fit(texts_train)\n",
    "num_embeddings = len(sgt.token_list)\n",
    "test_model = W2VSkipGram(num_embeddings, 64).to(device)\n",
    "optimizer = Adam(test_model.parameters())\n",
    "test_model.set_optimizer(optimizer)\n",
    "datagen = test_model.data_generator(texts_train, batch_size=1024, pool_size=5000)\n",
    "X_batch, y_batch, _ = next(datagen)\n",
    "X_batch = torch.from_numpy(X_batch).to(device)\n",
    "y_batch = torch.from_numpy(y_batch).to(device)\n",
    "%timeit -n 10 test_model.train_step(X_batch, y_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.3. Train Word Embeddings Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sgt = SkipGramTransformer().fit(texts_train)\n",
    "num_embeddings = len(sgt.token_list)\n",
    "embedding_dim = 64\n",
    "w2v_model = W2VSkipGram(num_embeddings, embedding_dim)\n",
    "optimizer = Adam(w2v_model.parameters())\n",
    "w2v_model.set_optimizer(optimizer)\n",
    "w2v_model.train(texts_train, 2, batch_display_interval=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.4. Save Word Embeddings Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Please comment your code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.5. Load Word Embeddings Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Please comment your code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2. Character Embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.1. Data Preprocessing for Character Embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*You are required to describe which preprocessing techniques were used with justification of your decision.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Please comment your code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.2. Build Character Embeddings Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*You are required to describe how hyperparameters were decided with justification of your decision.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Please comment your code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.4. Train Character Embeddings Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Please comment your code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.5. Save Character Embeddings Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Please comment your code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.6. Load Character Embeddings Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Please comment your code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3. Sequence model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.1. Apply/Import Word Embedding and Character Embedding Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*You are required to describe how hyperparameters were decided with justification of your decision.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Please comment your code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.2. Build Sequence Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*You are required to describe how hyperparameters were decided with justification of your decision.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Please comment your code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.3. Train Sequence Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Please comment your code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.4. Save Sequence Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Please comment your code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.5. Load Sequence Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Please comment your code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 - Evaluation\n",
    "\n",
    "(*Please show your empirical evidence*)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1. Performance Evaluation\n",
    "\n",
    "\n",
    "You are required to provide the table with precision, recall, f1 of test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Please comment your code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2. Hyperparameter Testing\n",
    "*You are required to draw a graph(y-axis: f1, x-axis: epoch) for test set and explain the optimal number of epochs based on the learning rate you have already chosen.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Please comment your code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Object Oriented Programming codes here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*You can use multiple code snippets. Just add more if needed*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If you used OOP style, use this section"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}